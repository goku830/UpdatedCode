import pandas as pd
import numpy as np
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, LeaveOneGroupOut
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import accuracy_score, roc_auc_score
import warnings
warnings.filterwarnings("ignore")

# =========================
# 1. Load Data
# =========================
file_path = r"C:\Users\brand\Desktop\trial1.xlsx"
df = pd.read_excel(file_path)

groups = df['species'].values
labels = df['label'].values
feature_df = df.drop(columns=['species', 'Source', 'label'])

# =========================
# 2. Output Directory
# =========================
output_dir = r"C:\Users\brand\Desktop\FilteredFeaturedAD4"
os.makedirs(output_dir, exist_ok=True)

# =========================
# 3. Threshold grids
# =========================
zero_grid = [0.4, 0.5, 0.6, 0.7]
mean_grid = [0, 5, 10, 20, 30,]
std_grid  = [0, 5, 10, 20, 30,]

# =========================
# 4. RF Hyperparameters
# =========================
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt']
}

results = []

# =========================
# 5. Loop Over Thresholds
# =========================
for zero_thresh in zero_grid:
    for mean_thresh in mean_grid:
        for std_thresh in std_grid:

            print(f"\nTesting: Zero≤{int(zero_thresh*100)}% | "
                  f"Mean≥{mean_thresh} | STD≥{std_thresh}")

            logo = LeaveOneGroupOut()
            fold_accuracies = []
            y_true_all = []
            y_probs_all = []

            for train_idx, test_idx in logo.split(feature_df, labels, groups):

                X_train_raw = feature_df.iloc[train_idx]
                X_test_raw = feature_df.iloc[test_idx]
                y_train = labels[train_idx]
                y_test = labels[test_idx]

                # ---- Prevalence filter (TRAIN ONLY)
                zero_fraction_train = (X_train_raw == 0).sum() / X_train_raw.shape[0]
                prevalence_mask = zero_fraction_train <= zero_thresh

                X_train_prev = X_train_raw.loc[:, prevalence_mask]
                X_test_prev = X_test_raw.loc[:, prevalence_mask]

                if X_train_prev.shape[1] == 0:
                    continue

                # ---- Mean / STD filter (TRAIN ONLY)
                means_train = X_train_prev.mean()
                stds_train = X_train_prev.std()

                keep_mask = (means_train >= mean_thresh) & (stds_train >= std_thresh)

                X_train_filtered = X_train_prev.loc[:, keep_mask]
                X_test_filtered = X_test_prev.loc[:, keep_mask]

                if X_train_filtered.shape[1] == 0:
                    continue

                # ---- VarianceThreshold
                vt = VarianceThreshold(threshold=0.0)
                X_train_vt = vt.fit_transform(X_train_filtered)
                X_test_vt = vt.transform(X_test_filtered)

                if X_train_vt.shape[1] == 0:
                    continue

                # ---- Inner CV hyperparameter tuning
                rf = RandomForestClassifier(random_state=42)

                grid_search = GridSearchCV(
                    rf,
                    param_grid,
                    scoring='roc_auc',
                    cv=3,
                    n_jobs=-1
                )

                grid_search.fit(X_train_vt, y_train)
                best_params = grid_search.best_params_

                optimized_rf = RandomForestClassifier(
                    **best_params,
                    random_state=42
                )

                optimized_rf.fit(X_train_vt, y_train)

                # ---- Outer fold evaluation
                y_probs = optimized_rf.predict_proba(X_test_vt)[:, 1]
                y_pred = optimized_rf.predict(X_test_vt)

                fold_accuracies.append(accuracy_score(y_test, y_pred))
                y_true_all.extend(y_test)
                y_probs_all.extend(y_probs)

            # ---- Store results
            if len(fold_accuracies) > 0:
                mean_acc = np.mean(fold_accuracies)
                std_acc = np.std(fold_accuracies)
                auc = roc_auc_score(y_true_all, y_probs_all)

                results_row = {
                    "ZeroThresh": zero_thresh,
                    "MeanThresh": mean_thresh,
                    "STDThresh": std_thresh,
                    "Accuracy": mean_acc,
                    "AccuracySTD": std_acc,
                    "AUC": auc
                }

                print(f"Accuracy: {mean_acc:.3f} ± {std_acc:.3f} | AUC: {auc:.3f}")

# =========================
# 6. Save Summary Results
# =========================
results_df = pd.DataFrame(results)
results_df = results_df.sort_values("AUC", ascending=False)

summary_path = os.path.join(output_dir, "threshold_results_summary.csv")
results_df.to_csv(summary_path, index=False)

print("\n==============================")
print("ALL RESULTS SAVED TO:")
print(summary_path)
print("==============================")
print(results_df)
